# HTML HW2 資工二 嚴得愷 B12902076

## Problem 5

### **My Insight：**

I don't fully agree with the argument claimed by ChatGPT. First of all, we focus on the method ze propose. What we already got is $N-1$ terms, but ze tries to solve the N-degree linear system. Zir method is undoubtedly impossible because we need more term to exactly figure out the value of each coefficient of the polynomial. However, the idea of solving the linear equation is seemingly feasible. Although we can't figure out the exact value, but we can use the general solution to estimate the next term may follow what mathematical formulas. Moreover, we don't even know the format of input. As a consequence, if the input is not ordinary, it's nearly impossible to predict thenext term. After discussion from different viewpoints, we find it extremely difficult to complete the prediction. In conclusion, we need one more term to use this polynomial method to estimate the next term of the sequence, so the strategy ChatGPT proposed is not correct.

## problem 6

By observation, we know that the 16 numbers can be divided into 4 groups.

1. {1, 3, 5, 7} will be pure green if all of the lottery tickets are **type A or type D**
2. {2, 4, 6, 8} will be pure green if all of the lottery tickets are **type B or type D**
3. {9, 11, 13, 15} will be pure green if all of the lottery tickets are **type A or type C**
4. {10, 12, 14, 16} will be pure green if all of the lottery tickets are **type B or type D**

Our goal is to calculate the probability of meeting one of any groups' need to win the prize. My strategy can be illustrated easily by the figure below. In order to calculate the union probability, some overlap cases should be carefully dealt with.
The probabilty of the event happens is :
$$
p(all\,\,tickets\,\,are\,\,pure\,\,AC, BC, AD\,\,or\,\,BD)-p(all\,\,tickets\,\,are\,\,pure\,\,A,B,C\,\,or\,\,D)
$$
which can be easily computed as :
$$
4\cdot(\frac{2}{4})^5-4\cdot(\frac{1}{4})^5=\frac{31}{256}
$$
![image](https://hackmd.io/_uploads/BkSm9PhCC.png)

## problem 7

In such cases, the probability is to consecutively get tickets of type A or type D. So the probability is :
$$
(\frac{2}{4})^5=\frac{1}{32}
$$
## Problem 8

The probability $p$ we want to compute is :
$$
\begin{aligned}
    p &= P(\forall{m}\in\{1,2,...,M\}\,and\,\,t\in\{M+1,M+2,...\}\,\,\,\mu_m\le\frac{c_m}{N_m}+\sqrt{\frac{ln(t)+ln(M)-\frac{1}{2}ln(\delta)}{N_m}}) \\
    &\ge1-\sum^\infty_{t=M+1}\sum^M_{m=1}{P(\mu_m>\frac{c_m}{N_m}+\sqrt{\frac{ln(t)+ln(M)-\frac{1}{2}ln(\delta)}{N_m}})}\\
    &\ge1-\sum^\infty_{t=M+1}\sum^M_{m=1}{P(\mu_m>\frac{c_m}{N_m}+\sqrt{\frac{ln(t)-\frac{1}{2}ln(\delta)}{N_m}})}\\
    &\ge1-\sum^\infty_{t=M+1}\sum^M_{m=1}(\delta{t^{-2}})\\
    &=1-\sum^\infty_{t=M+1}M\delta{t^{-2}}\\
    &=1-(\sum^\infty_{t=1}M\delta{t^{-2}}-\sum^M_{t=1}M\delta{t^{-2}})\\
    &\ge1-{\delta}\cdot\frac{M\pi^2}{6}\,\,\,\,\,(since\,\,M\ge2\,\,\,\,we\,\,have\,\,\frac{M\pi^2}{6}>1)\\
    &>1-\delta
\end{aligned}
$$
## Problem 9

Since we know the dimension of $h$ is $k$  and we only need to consider the number of $1s$ in $h$ , we can at most $k+1$ different outputs (from $0$ to $k$ ). As a result, it can perfectly shatter $k+1$ datas. Also, it's not capable of shattering $k+2$ data because in such case at least two of the data will share the same "number of $1s$ ", which means $k+2$ is a break point. Hence, we know that the VC dimension of the set of all symmetric boolean functions is $k+1$ .
## Problem 10

First, we consider the case of $s=+1$ .Suppose there isn't any noise added to the label $y$ , then we can compute the the probability of the hypothesis to correctly predict $f(x)$ .
In this case, the probability of **making wrong decision** is $P(x\,\,is\,\,between\,\,0\,\,and\,\,\theta)=P(sign(x-\theta){\cdot}sign(x)\le0)=\frac{|\theta|}{2}$ . As a result, the probability of **making correct decision** is $P(sign(x-\theta){\cdot}sign(x)>0)=1-\frac{|\theta|}{2}$ . Then we consider the effect of the noise. What we want is $E_{out}(h_{1, theta})$ , and the calculation is as follow :
$$
\begin{aligned}
E_{out}(h_{1, theta}) &=P(sign(x-\theta){\cdot}sign(x)>0)\,\cdot\,p\,+\,P(sign(x-\theta){\cdot}sign(x)\le0)\,\cdot\,(1-p)\\
	&=(1-\frac{|\theta|}{2})\cdot{p}+\frac{|\theta|}{2}\cdot(1-p)\\
	&=p+(\frac{1}{2}-p)|\theta|\\
	&then\,\,we\,\,have\,\,v=\frac{1}{2}-p=s(\frac{1}{2}-p)\,\,,\,\,u=p=\frac{1}{2}-v\,.
\end{aligned}
$$
As for the case of $s=-1$ , we use similar method. In this case, the probability of **making correct decision** is $P(x\,\,is\,\,between\,\,0\,\,and\,\,\theta)=P(sign(x-\theta){\cdot}sign(x)\le0)=\frac{|\theta|}{2}$ . As a result, the probability of **making wrong decision** is $P(sign(x-\theta){\cdot}sign(x)>0)=1-\frac{|\theta|}{2}$ . Then we consider the effect of the noise. What we want is $E_{out}(h_{-1, theta})$ , and the calculation is as follow :
$$
\begin{aligned}
E_{out}(h_{-1, theta}) &=P(sign(x-\theta){\cdot}sign(x)>0)\,\cdot\,(1-p)\,+\,P(sign(x-\theta){\cdot}sign(x)\le0)\,\cdot\,p\\
	&=(1-\frac{|\theta|}{2})\cdot{(1-p)}+\frac{|\theta|}{2}\cdot{p}\\
	&=1-p+(-\frac{1}{2}+p)|\theta|\\
	&then\,\,we\,\,have\,\,v=-\frac{1}{2}+p=-(\frac{1}{2}-p)=s(\frac{1}{2}-p)\,\,,\,\,u=1-p=\frac{1}{2}-v\,.
\end{aligned}
$$

By combining the conclusion of two cases, we proved that for any $h_{s, \theta}$ with $s\in\{+1,-1\}\,,\theta\in[1,-1]$ , we have :
$$
E_{out}(h_{s,\theta})=u+v\cdot|\theta|\,\,,\,\, where\,\,v=s(\frac{1}{2}-p)\,\,,\,\,u=\frac{1}{2}-v
$$
## Problem 11

I use Python to implement the algorithm and conduct the experiments with the generated data with $p=0.15$ .The scatter plot of $(E_{in}(g),\,E_{out}(g))$ is as the following figure. And the median of $E_{in}(g)-E_{out}(g)$ is $0.12795570132595213$
![image](https://hackmd.io/_uploads/Hy_oVCRRC.png)
![image](https://hackmd.io/_uploads/Sk70VRARC.png)

## Problem 12

I use the similar way to implement the algorithm, just change the method to get $g$ into a random way. The corresponding scatter plot is as follow. And the median of $E_{out}(g_{RND})-E_{in}(g_{RND})$ is $0.003174867920442165$
Comparing two plot, we can easily find that the overall $E_{in}$ and $E_{out}$ of question 11 are lower than question 12. It's probably because in question 11 we use a more "smart" algorithm to choose the seemingly best $g$ , instead of randomly generate $g_{RND}$. Moreover, we can obersve that $E_{in}(g_{RND})$ and $E_{out}(g_{RND})$ sometimes even bigger than $0.5$, which means this method sometimes may worse than simply guessing. Also, in the plot of question 12 we can easily observe the connection between $E_{in}$ and $E_{out}$, which is mentioned in the lecture. As for **the median difference between question 11 and question 12**, we observe that the former's median is significantly larger than the latter's. In my opinion, it's possibly because in question 11 we choose the best $g$ (a.k.a. the one with smallest $E_{in}$), so by the knowledge from the lecture, we know that the gap between $E_{in}$ and $E_{out}$ will be larger. On the other hand, we just randomly pick $g_{RND}$ without choosing, so the gap between $E_{in}$ and $E_{out}$ will be smaller.

![image](https://hackmd.io/_uploads/B1zJBCC0C.png)
![image](https://hackmd.io/_uploads/Byyfr0RCR.png)

## Problem 13 

In order to find an upper bound of the VC dimension, we need to prove that number can't be shattered by the hypothesis. In an ideal case, the model of every dimension can classify the data into two groups. And we have $d$ data, which means we only need at most ${\lfloor}log_2(d)\rfloor$ dimensions to do so. In addition, we also need to take the case of "all +1" and "all -1" into consider. So I think the upper bound is $2+{\lfloor}log_2(d)\rfloor$ .
Since in each dimension the $\theta$ can be placed at one of the $d-1$ interval, and we have $d$ dimensions and we have $s\in\{+1,-1\}$. Also, we need to consider the case of "all +1" or "all -1", So we have at most $2d(d-1)+2$ data can be classified. Since :
$$
\begin{aligned}
log_2(2d(d-1)+2)&=log_2(2d^2-2d+2)\\
	&=2+log_2(\frac{d^2-d+1}{2})\\
	&{\ge}2+\lfloor{log_2(d)}\rfloor\,\,\,\,for\,\,\,d>\frac{3+\sqrt{5}}{2}
\end{aligned}
$$
So, we can infer that :
$$
2d(d-1)+2{\ge}2^{2+{\lfloor}log_2(d)\rfloor}
$$
which means $2+{\lfloor}log_2(d)\rfloor$ is a valid VC dimension.
By careful inspection, the case of $d=1$ and $d=2$ also works. As a result, the upper bound is as follows :
$$
\begin{cases}
2\,\,,&if\,\,d=1\\
3\,\,,&if\,\,d=2\\
2+{\lfloor}log_2(d)\rfloor\,\,,&if\,\,d\,\,{\ge}\,\,3\\
\end{cases}
$$